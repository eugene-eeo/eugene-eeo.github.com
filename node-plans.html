<html>
<head>
<style type="text/css">
* {
	font-family: monospace!important;
	font-size: 1em;
	outline: none;
	color: #222;
}
body {
	width: 650px;
	margin-left: auto;
	margin-right: auto;
	margin-top: 0.5em;
}
a {
	color: #3b5998;
	text-decoration: none;
}
h1 {
	font-size: 20pt;
	margin-bottom: 0px;
}
h2 {
	font-size: 16pt;
	margin-bottom: 8px;
}
h3 {
	font-size: 13pt;
	margin-bottom: 0px;
}
table {
	width: 100%;
}
span {width: 100%; display: inline;}
td {
	width: 100%;
	background-color: #E8E8E8;
}
p {text-align: justify;}
</style>
<title>XMLRPC Node</title>
</head>
<body>
<table width="100%">
	<tr><td width="100%" style="background-color: #3b5998; height:5px"></td></tr>
	<tr><td width="100%">[<a href="index.html">Back to Index</a>] UUID: fb610050-a19a-11e2-9e96-0800200c9a66</td></tr>
	<tr><td width="100%">Type: Concept [Infancy]</td></tr>
</table>
<center>
<h3>THE XMLRPC BASED NODE</h3>
CONCEPT AND PRACTICAL EXAMPLES
<pre>





<a href="node/node.py">http://eugene-eeo.github.com/node/node.py</a>
April 10, 2013



First Edition (No Revision)




Referral: eugene-eeo.github.com
Contact: packwolf58@gmail.com
</center>
</pre>


<center><b>ABOUT</b></center>
<p>
The XMLRPC-based node was meant to be an implementation, or a working example of the BareMetal Node concept, see <a href="http://www.returninfinity.com/baremetalnode.html">http://www.returninfinity.com/baremetalnode.html</a> for the concept. There are several key differences, however, for example: the XMLRPC Node is coded in Python, while the BareMetal Node is probably coded in some other lower level languages.
<p>
The XMLRPC-based Node was coded hastily after a quick plan which was drawn on a piece of paper- a form of enlightment. Therefore, the examples provided may not be of excellent quality in terms of documentation. Documentation used and the modules are sourced from <a href="http://code.activestate.com/recipes/81549-a-simple-xml-rpc-server/">ActiveState</a>, and <a href="http://docs.python.org/2/library/simplexmlrpcserver.html">Python Documentation</a>.
<p>
Components which make up the Node are as follows: a server framework, where functions are referenced and data is stored, the controller node, which is the main controller for all of the other nodes and can control the server (i.e. admin), and multiple worker nodes, which tasks are dispatched to. All of the nodes require access to the main server framework. There is no form of data storage onboard the main server framework (currently, future releases would include something like a database, possibly utilizing CrouchDB or SQl).
<p>
	<pre>
      Control Node
           |             +-- Node 1 -> Gets the data; Parses and executes; Tells the server
  Server --+----XMLRPC---+-- Node 2    that it's finished.
                         +-- Node 3
    </pre>
</p>


<center><b>SERVER</b></center>
<p>
While being technically referred to as the "server framework", the server is actually the backbone of XMLRPC Node- it is the place where all other Nodes come together and reference data, and is the biggest file (2399 bytes) amongst the other components. If it is to be practically implemented for a large scale processing frame, it must be hosted in a place where requests are averagely above 10/s (depending on how many nodes you have, as the nodes would constantly poll the server for any changes within the node's task stack- we'll come to that later).
<p>
The server's layout is of a classical Python XMLRPC Server-style layout- there is a class object, and then the class object would be registered and used. That is precisely the case here. Functions are wrapped into the server's main class, which is named NodeFunctions for the time being- it'll make it easier for me to apply some extraneous changes in the future. The server contains a few relatively easy and simple functions, namely:
<p>
These functions are for the controller node, with "node" being the node that the controller wants to control or create, and "node_id" being the identity/UUID of the controller node, which will be matched to the one onboard the server to prevent lots of controllers from taking over:
<ul>
	<li>create_node(node, node_id): which lets the controller node create a worker node.</li>
	<li>register_control(node_id): registers a controller node.</li>
	<li>load_task(node, task, node_id): lets the controller node load a task into a worker node.</li>
	<li>kill_node(node, node_id): kills or destroys a node.</li>
</ul>
<p>
These are for the worker node, with "node" being the node's name, i.e. worker1:
<ul>
	<li>finish(node, task): tells the server that the current node has finished something, and unloads that task.</li>
	<li>get_tasks(node): gets the tasks for the current node.</li>
</ul>
<p>
Currently, the authentication methods aren't very complex or secure- so there is absolutely no guarantee if you do use or implement this software in your infrastructure. The only authentication method is to compare the controller node's "node_id" variable to the "control_node" variable onboard the server. The hosting section is provided via the server = SimpleXMLRPCServer.SimpleXMLRPCServer(("localhost", 8000)) and the server.serve_forever() function, which should give you a log of what's happening in your server. Be sure not to package it in a "try and except anything" loop, as the only way to stop it is via ^C, which raises KeyboardInterrupt.
<p>
There is alot that I can do to improve the overall security of the server software, i.e. security mechanisms or functions and other authentication functions, but technically and finally, it all comes down to the security of your server itself- and not the software, or it would fall into the "bloated software" category, and would increase the overall complexity.



<center><b>CONRTOLLER</b></center>
<p>
The controller script is actually a script which connects to the server, and offers the user the complete control over the server. There is currently no UI/interface for the controller- it's a basic script. However, one could read the source code of the server program and then determine what to add or remove from the controller script. For what I've used during the testing phase, functions that are needed are added into the script and saved, and then ran using the command-line.
<p>
In the ecosystem of the XMLRPC Node, the controller must be connected to the server, and then commands are executed, aka functions are called. The controller does not have access to every variable onboard the server, but every Node onboard it. It is also considered a Node, though it's technically more of a master instead of a servant. Whether you can have multiple controllers is unsure (as all controllers, even registered, would have the name "control-node")- but I will try to implement that feature with permissions over which nodes, etc. If we refer this to a proxy, it's something like so:
<pre>
 +---------+      /-=-=-=-=\      +--------+
 | Control | ===> | Server | ===> | Worker |
 |  Node   | <=== |        | <=== |  Node  |
 +---------+      \=-=-=-=-/      +--------+
</pre>
<p>
The controller must have a specific string in place when it's calling the features that only a controller node can access- it must supply the node_id variable. That variable would be matched to another, presumably identical one onboard the server. If the matching fails, the node will not have access to anything. This variable is usually referred to as the nodeID variable, both in the source and in this documentation.
<p>
It is recommended that you create a UI to the controller node, and catch exceptions and values that return None or False, just to give you a broader view of what's happening inside the server. You should also add the "print" statement to the front of every function that you call, as it would show in stdout whether the function call was successful or not.



<center><b>WORKER</b></center>
<p>
Worker Nodes are Nodes that help you process something, and are at the receiving end of what the server and Controller Node have done. The Worker Node is practically a script which connects to the server, and then executes the commands or tasks, then tells the server "I have finished!", which removes the task and clears the queue. While the Worker Node actually needs to get data from the server every now and then, it's recommended that you do something like the following:
<pre>
	new = server.get_tasks(node_name)
	for item in new:
		subprocess.Popen(item, shell=True)
		server.finish(node_name, item)
	time.sleep(1.0)
</pre>
And prolong the delay between successive requests to the server, as it would reduce the server load and reduces the risks of overheating on both server and client sides, but also decreases the availability of the Node, as the "reloading time" increases, much like any weapon. So you should probably strike a balance between the availability and sleeping time (time.sleep), i.e. allowing critical Nodes to have a lower sleeping time and minor Nodes to have a higher sleeping time. As you can see, the get_tasks function actually returns a list, which is stacked up like so:
<pre>
   [ Task  4 ]  <=== FROM SERVER <=== Tasks Loaded By Controller Node
        |
        |                                                        TO SERVER
   [ Task  3 ]                                                       |
   [ Task  2 ]    =====> [ Task 1 ] ==executed=by=Node==> server.finish(node, task)
\={Worker Node}=/
</pre>
<p>
There is actually not much programming on the server side, but rather on the client-side, and that gives users and developers alike three major advantages; first, it allows the server admin to pre-configure Nodes to match their personal preferences and needs which seems to be the most valuable aspect in all software. Secondly, it allows the developers to easily develop front-ends for the XMLRPC Node. Thirdly, it also allows you to customize XMLRPC Node even further- which correlates to the first reason.

<p>
<table width="100%">
	<tr><td width="100%">[<a href="index.html">Back to Index</a>] Authored: Wed Apr 10 14:50:16 2013</td></tr>
	<tr><td width="100%" style="background-color: #3b5998; height:5px"></td></tr>
</table>