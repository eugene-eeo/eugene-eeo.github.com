<!doctype html><meta charset=utf-8><style>body{font-family:Georgia,serif;font-size:16px;width:35em;margin:3em auto}code,pre{font-family:Inconsolata,Menlo,Consolas,monospace}li{margin:.5em 0}.has-jax{font:inherit;font-size:90%}h1,h2,h3{font-weight:400}hr{background-color:#000;border:0;height:1px}.footnotes{margin-top:5em;font-size:.75em}table{border-collapse:collapse}th{background-color:#EEE}td,th{border:1px solid #000;padding:.2em .5em}</style><title>Root Mean Squared Error</title><h1>Root Mean Squared Error</h1><p>Given two sequences <code>$ x_1 \ldots x_n $</code> and <code>$ y_1 \ldots y_n $</code>, (usually one is predicted results using some model and the other is the &lsquo;real&rsquo; results,) the Root Mean Squared Error (RMSE) between them is given by:<p><code>$$ \sqrt{\frac{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots + (x_n - y_n)^2}{n}} $$</code><p>Larger RMSE means more errors. However very small losses in the RMSE can mean large gains in accuracy of the model. Logic behind it: assume that our model constantly predicts results with <code>$ \pm \epsilon $</code> of error:<p><code>$$ y_i = f(x_i) = x_i \pm \epsilon $$</code><p>Then the RMSE is:<p><code>$$ \sqrt{\frac{\epsilon^2 + \epsilon^2 + \ldots}{n}} = \sqrt{\epsilon^2} = \epsilon $$</code><p>Then if we improve the model by decreasing the error by some amount <code>$ \delta \epsilon $</code>, the RMSE decreases by the same amount. It is exponentially harder to achieve more and more accurate results.<p><strong>Intuition:</strong> In a dataset of <code>$ N $</code> rows, our probabilistic model selects <code>$ n $</code> rows with a uniform distribution. Then take <code>$ K $</code> to be the total number of meaningful rows in the dataset. Then the probability of getting <code>$ r $</code> meaningful data follows <a href=https://en.wikipedia.org/wiki/Hypergeometric_distribution>hypergeometric</a> distribution, since the number of meaningful rows decreases with each pick:<p><code>$$ \frac{{K \choose r} {N - K \choose n - r}}{N \choose n} $$</code><p>Then we can see that getting more and more accurate results (by processing meaningful rows) in a large population <code>$ N $</code>, where <code>$ n $</code> is bounded by computing power and time is exponentially harder:<p><code>$$ \text{F}(r+1) - \text{F}(r) = \Pr(R=r+1) $$</code></p><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js></script><script>!function(){function e(e){var t=e.match(a);return t?{TeX:t[1]||t[2],displayMode:!!t[1]}:null}for(var t=document.getElementsByTagName("code"),a=/(?:^\$\$\s*([\s\S]+)\s*\$\$$)|(?:^\$\s*([\s\S]+)\s*\$$)/,s=t.length;s--;){var n=t[s],r=e(n.textContent);r&&(katex.render(r.TeX,n,{displayMode:r.displayMode,throwOnError:!1}),n.classList.add("has-jax"))}}()</script>